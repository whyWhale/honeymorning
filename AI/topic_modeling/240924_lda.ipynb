{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from itertools import zip_longest\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "\n",
    "pyLDAvis.enable_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(file_path):\n",
    "    \"\"\"\n",
    "    file_path 경로에서 불용어를 불러오는 함수\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords = f.read().splitlines()\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "def preprocess_text(text, stopwords=None):\n",
    "    \"\"\"\n",
    "    Okt를 사용한 텍스트 전처리 함수\n",
    "    - 불용어 제거\n",
    "    - 명사 추출\n",
    "    \"\"\"\n",
    "    okt = Okt()\n",
    "    # 정규 표현식을 사용한 숫자/특수문자 제거\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    nouns = okt.nouns(text)\n",
    "\n",
    "    if stopwords:\n",
    "        nouns = [word for word in nouns if word not in stopwords]\n",
    "    \n",
    "    # 길이가 1인 단어들 제거\n",
    "    nouns = [word for word in nouns if len(word) > 1]\n",
    "\n",
    "    processed_text = ' '.join(nouns)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "def load_and_merge_section_data(section_number, hours_back=9):\n",
    "    \"\"\"\n",
    "    섹션 번호에 해당하는 모든 pkl 파일을 불러와서 기사 본문을 개별 문서로 병합하는 함수\n",
    "    현재 시간에서 -hours_back 시간 전까지의 파일만 병합\n",
    "    \"\"\"\n",
    "    current_time = datetime.now()\n",
    "    start_time = current_time - timedelta(hours=hours_back)\n",
    "\n",
    "    valid_hours = []\n",
    "    for hour in range(hours_back + 1):  # 0부터 hours_back까지의 시간을 계산\n",
    "        valid_hour = (start_time + timedelta(hours=hour)).strftime('%H')\n",
    "        valid_hours.append(valid_hour)\n",
    "\n",
    "    file_patterns = [f\"./data/{section_number}/*_{hour}.pkl\" for hour in valid_hours]\n",
    "\n",
    "    merged_content = []\n",
    "    for file_pattern in tqdm(file_patterns, desc=f\"섹션 번호: {section_number} 파일\"):\n",
    "        file_list = glob.glob(file_pattern)\n",
    "\n",
    "        for file_path in file_list:\n",
    "            print(file_path)\n",
    "            with open(file_path, 'rb') as file:\n",
    "                data = pickle.load(file)\n",
    "                for article in data:\n",
    "                    merged_content.append(article['content'])\n",
    "\n",
    "    print(\"merge len: \" + str(len(merged_content)))\n",
    "\n",
    "    return merged_content  # 각각의 기사를 리스트 형태로 반환\n",
    "\n",
    "def visualize_lda_model(lda_model, corpus, dictionary):\n",
    "    vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "    return vis\n",
    "\n",
    "# def process_sections(section_numbers, stopwords_file_path, hours_back=9):\n",
    "#     \"\"\"\n",
    "#     주어진 섹션 번호 리스트에 대해 각 섹션별로 데이터를 병합하고 전처리하는 함수\n",
    "#     \"\"\"\n",
    "#     # 불용어 로드\n",
    "#     print(\"불용어를 불러오는 중입니다...\")\n",
    "#     stopwords = load_stopwords(stopwords_file_path)\n",
    "\n",
    "#     processed_documents = []\n",
    "\n",
    "#     for section_number in tqdm(section_numbers, desc=\"섹션 처리중\"):\n",
    "#         # 데이터 병합 (각 기사별로 처리)\n",
    "#         merged_content_list = load_and_merge_section_data(section_number, hours_back)\n",
    "\n",
    "#          # 각 기사를 개별 문서로 전처리\n",
    "#         for content in merged_content_list:\n",
    "#             processed_text = preprocess_text(content, stopwords)\n",
    "#             processed_documents.append()\n",
    "#             processed_documents.append(processed_text.split())  # 리스트 형태로 추가\n",
    "\n",
    "#     return processed_documents  # 문서 리스트 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_data_for_lda(doc):\n",
    "#     \"\"\"\n",
    "#     LDA를 위한 데이터를 준비하는 함수\n",
    "#     - doc: 전처리된 텍스트 데이터 (단일 섹션의 텍스트)\n",
    "#     \"\"\"\n",
    "#     # 전처리된 문서를 리스트로 변환\n",
    "#     text = doc.split()\n",
    "\n",
    "#     # 딕셔너리 생성\n",
    "#     dictionary = corpora.Dictionary([text])\n",
    "\n",
    "#     # 코퍼스 생성(단어의 빈도수로 변환된 텍스트 데이터)\n",
    "#     corpus = [dictionary.doc2bow(text)]\n",
    "\n",
    "#     return dictionary, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "섹션 번호: 100 파일: 100%|██████████| 10/10 [00:00<00:00, 5010.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/100\\2024-09-24_11.pkl\n",
      "merge len: 171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "섹션 번호: 101 파일: 100%|██████████| 10/10 [00:00<00:00, 1219.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/101\\2024-09-24_11.pkl\n",
      "merge len: 720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "섹션 번호: 102 파일: 100%|██████████| 10/10 [00:00<00:00, 1238.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/102\\2024-09-24_11.pkl\n",
      "merge len: 681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "섹션 번호: 103 파일: 100%|██████████| 10/10 [00:00<00:00, 1329.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/103\\2024-09-24_11.pkl\n",
      "merge len: 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "섹션 번호: 104 파일: 100%|██████████| 10/10 [00:00<00:00, 19645.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/104\\2024-09-24_11.pkl\n",
      "merge len: 166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "섹션 번호: 105 파일: 100%|██████████| 10/10 [00:00<00:00, 1139.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/105\\2024-09-24_11.pkl\n",
      "merge len: 83\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "section_numbers = [100, 101, 102, 103, 104, 105]\n",
    "stopwords_file_path = './data/korean_stopwords.txt'\n",
    "\n",
    "\n",
    "section_documents = []\n",
    "\n",
    "for section_number in section_numbers:\n",
    "    stopwords = load_stopwords(stopwords_file_path)\n",
    "\n",
    "    processed_documents = []\n",
    "\n",
    "    merged_content_list = load_and_merge_section_data(section_number, hours_back=9)\n",
    "\n",
    "    for content in merged_content_list:\n",
    "        processed_text = preprocess_text(content, stopwords)\n",
    "        processed_documents.append(processed_text.split())\n",
    "\n",
    "    section_documents.append(processed_documents)\n",
    "    \n",
    "\n",
    "print(len(section_documents))\n",
    "\n",
    "\n",
    "\n",
    "# processed_docs = process_sections(section_numbers, stopwords_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100섹션의 토픽들\n",
      "(0, '0.013*\"대통령\" + 0.012*\"민주당\" + 0.011*\"의원\" + 0.010*\"대표\" + 0.009*\"여사\" + 0.009*\"국민\" + 0.007*\"후보\" + 0.007*\"공천\" + 0.007*\"국회\" + 0.007*\"김건희\"')\n",
      "(1, '0.021*\"대표\" + 0.018*\"독대\" + 0.017*\"대통령\" + 0.012*\"국민\" + 0.011*\"대통령실\" + 0.008*\"만찬\" + 0.008*\"금투세\" + 0.008*\"의원\" + 0.007*\"요청\" + 0.007*\"북한\"')\n",
      "(2, '0.015*\"지금\" + 0.013*\"대표\" + 0.012*\"대통령\" + 0.011*\"통일\" + 0.010*\"얘기\" + 0.009*\"북한\" + 0.007*\"주장\" + 0.007*\"국민\" + 0.007*\"대해\" + 0.006*\"의원\"')\n",
      "101섹션의 토픽들\n",
      "(0, '0.005*\"사업\" + 0.005*\"개발\" + 0.004*\"가구\" + 0.004*\"예정\" + 0.004*\"제공\" + 0.004*\"미국\" + 0.004*\"분양\" + 0.003*\"브랜드\" + 0.003*\"투자\" + 0.003*\"서울\"')\n",
      "(1, '0.008*\"금리\" + 0.008*\"시장\" + 0.007*\"미국\" + 0.006*\"가격\" + 0.006*\"증가\" + 0.005*\"지수\" + 0.005*\"인하\" + 0.005*\"주가\" + 0.005*\"상승\" + 0.005*\"투자\"')\n",
      "(2, '0.012*\"고려아연\" + 0.010*\"기업\" + 0.007*\"투자\" + 0.006*\"기술\" + 0.006*\"경영\" + 0.006*\"사업\" + 0.006*\"회장\" + 0.005*\"파트너\" + 0.005*\"반도체\" + 0.004*\"회사\"')\n",
      "102섹션의 토픽들\n",
      "(0, '0.006*\"병원\" + 0.006*\"서울\" + 0.005*\"지역\" + 0.004*\"의원\" + 0.004*\"환자\" + 0.004*\"지난\" + 0.003*\"올해\" + 0.003*\"기자\" + 0.003*\"의료\" + 0.003*\"사업\"')\n",
      "(1, '0.008*\"병원\" + 0.008*\"당첨\" + 0.005*\"지난\" + 0.004*\"한국\" + 0.004*\"서울\" + 0.004*\"아내\" + 0.003*\"경우\" + 0.003*\"대한\" + 0.003*\"의료\" + 0.003*\"의견\"')\n",
      "(2, '0.015*\"경찰\" + 0.008*\"사건\" + 0.008*\"혐의\" + 0.007*\"지난\" + 0.007*\"사고\" + 0.006*\"수사\" + 0.006*\"여사\" + 0.006*\"검찰\" + 0.005*\"조사\" + 0.005*\"남성\"')\n",
      "103섹션의 토픽들\n",
      "(0, '0.018*\"년생\" + 0.009*\"사람\" + 0.007*\"생각\" + 0.005*\"마음\" + 0.004*\"계단\" + 0.004*\"수도\" + 0.004*\"문제\" + 0.004*\"시기\" + 0.003*\"경우\" + 0.003*\"상황\"')\n",
      "(1, '0.009*\"시인\" + 0.007*\"기온\" + 0.006*\"서울\" + 0.005*\"날씨\" + 0.004*\"박영근\" + 0.004*\"조혜영\" + 0.004*\"위험\" + 0.004*\"질환\" + 0.003*\"오늘\" + 0.003*\"때문\"')\n",
      "(2, '0.007*\"치료\" + 0.006*\"단백질\" + 0.005*\"기능\" + 0.005*\"교수\" + 0.005*\"섭취\" + 0.004*\"관련\" + 0.004*\"인지\" + 0.004*\"전기차\" + 0.004*\"기억\" + 0.004*\"판매\"')\n",
      "104섹션의 토픽들\n",
      "(0, '0.025*\"트럼프\" + 0.020*\"대통령\" + 0.018*\"해리스\" + 0.016*\"후보\" + 0.013*\"미국\" + 0.012*\"대선\" + 0.010*\"부통령\" + 0.007*\"공화당\" + 0.007*\"민주당\" + 0.007*\"경합주\"')\n",
      "(1, '0.015*\"금리\" + 0.011*\"달러\" + 0.009*\"미국\" + 0.008*\"중국\" + 0.007*\"상승\" + 0.007*\"우크라이나\" + 0.007*\"시장\" + 0.007*\"인하\" + 0.006*\"인텔\" + 0.006*\"테슬라\"')\n",
      "(2, '0.025*\"이스라엘\" + 0.020*\"헤즈볼라\" + 0.019*\"레바논\" + 0.011*\"공습\" + 0.010*\"미국\" + 0.008*\"이스라엘군\" + 0.008*\"이란\" + 0.007*\"지난\" + 0.007*\"전쟁\" + 0.006*\"공격\"')\n",
      "105섹션의 토픽들\n",
      "(0, '0.012*\"심장\" + 0.009*\"우주\" + 0.007*\"지구\" + 0.007*\"소행성\" + 0.006*\"의료\" + 0.004*\"배터리\" + 0.004*\"경우\" + 0.004*\"교수\" + 0.004*\"환자\" + 0.004*\"발생\"')\n",
      "(1, '0.008*\"기술\" + 0.008*\"데이터\" + 0.006*\"아이폰\" + 0.006*\"서비스\" + 0.006*\"제공\" + 0.005*\"애플\" + 0.005*\"개발\" + 0.005*\"네이버\" + 0.005*\"제품\" + 0.005*\"모델\"')\n",
      "(2, '0.012*\"게임\" + 0.009*\"매출\" + 0.008*\"국내\" + 0.007*\"구글\" + 0.007*\"코리아\" + 0.006*\"한국\" + 0.006*\"서비스\" + 0.006*\"억원\" + 0.006*\"의원\" + 0.004*\"카카오\"')\n"
     ]
    }
   ],
   "source": [
    "# 6개의 섹션에 대해 토픽 모델링 수행\n",
    "for i in range(len(section_numbers)):\n",
    "    dictionary = corpora.Dictionary(section_documents[i])\n",
    "    corpus = [dictionary.doc2bow(text) for text in section_documents[i]]\n",
    "\n",
    "    NUM_TOPICS = 3\n",
    "\n",
    "    lda_model = LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "    topics = lda_model.print_topics(num_words=10)\n",
    "    print(str(i+1 * 100) + \"섹션의 토픽들\")\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "\n",
    "    # 시각화 실행\n",
    "    visualization = visualize_lda_model(lda_model, corpus, dictionary)\n",
    "    pyLDAvis.save_html(visualization, f'lda_vis_{i+1*100}.html')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_TOPICS = 6\n",
    "\n",
    "# lda_model = LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "# topics = lda_model.print_topics(num_words=10)\n",
    "# for topic in topics:\n",
    "#     print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic",
   "language": "python",
   "name": "topic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
