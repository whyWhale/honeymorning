{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from itertools import zip_longest\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "pyLDAvis.enable_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(file_path):\n",
    "    \"\"\"\n",
    "    file_path 경로에서 불용어를 불러오는 함수\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords = f.read().splitlines()\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "def preprocess_text(text, stopwords=None):\n",
    "    \"\"\"\n",
    "    Okt를 사용한 텍스트 전처리 함수\n",
    "    - 불용어 제거\n",
    "    - 명사 추출\n",
    "    \"\"\"\n",
    "    okt = Okt()\n",
    "    # 정규 표현식을 사용한 숫자/특수문자 제거\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    nouns = okt.nouns(text)\n",
    "\n",
    "    if stopwords:\n",
    "        nouns = [word for word in nouns if word not in stopwords]\n",
    "    \n",
    "    # 길이가 1인 단어들 제거\n",
    "    nouns = [word for word in nouns if len(word) > 1]\n",
    "\n",
    "    processed_text = ' '.join(nouns)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "\n",
    "def load_and_merge_section_data(section_number, hours_back=8):\n",
    "    \"\"\"\n",
    "    섹션 번호에 해당하는 모든 pkl 파일을 불러와서 기사 본문을 개별 문서로 병합하는 함수\n",
    "    현재 시간에서 -hours_back 시간 전까지의 파일만 병합\n",
    "    \"\"\"\n",
    "    current_time = datetime.now()\n",
    "    start_time = current_time - timedelta(hours=hours_back)\n",
    "\n",
    "    valid_hours = []\n",
    "    for hour in range(hours_back + 1):  # 0부터 hours_back까지의 시간을 계산\n",
    "        valid_hour = (start_time + timedelta(hours=hour)).strftime('%H')\n",
    "        valid_hours.append(valid_hour)\n",
    "\n",
    "    file_patterns = [f\"./data/{section_number}/*_{hour}.pkl\" for hour in valid_hours]\n",
    "\n",
    "    merged_content = []\n",
    "    for file_pattern in tqdm(file_patterns, desc=f\"섹션 번호: {section_number} 파일\"):\n",
    "        file_list = glob.glob(file_pattern)\n",
    "\n",
    "        for file_path in file_list:\n",
    "            print(file_path)\n",
    "            with open(file_path, 'rb') as file:\n",
    "                data = pickle.load(file)\n",
    "                for article in data:\n",
    "                    merged_content.append(article['content'])\n",
    "\n",
    "    print(\"병합한 기사 개수: \" + str(len(merged_content)))\n",
    "\n",
    "    return merged_content  # 각각의 기사를 리스트 형태로 반환\n",
    "\n",
    "def visualize_lda_model(lda_model, corpus, dictionary):\n",
    "    vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "    return vis\n",
    "\n",
    "\n",
    "\n",
    "def process_documents(section_numbers, stopwords_file_path, hours_back=8):\n",
    "\n",
    "    stopwords = load_stopwords(stopwords_file_path)\n",
    "\n",
    "    processed_documents = []\n",
    "    \n",
    "    for section_number in section_numbers:\n",
    "        merged_content_list = load_and_merge_section_data(section_number, hours_back)\n",
    "        for content in merged_content_list:\n",
    "            processed_text = preprocess_text(content, stopwords)\n",
    "            processed_documents.append(processed_text.split())\n",
    "\n",
    "    return processed_documents\n",
    "\n",
    "\n",
    "def optimalize_lda_model(corpus, dictionary, processed_documents, start=2, end=6, step=1):\n",
    "    coherence_values = []\n",
    "    lda_model_list = []\n",
    "    for num_topics in range(start, end+1, step):\n",
    "        lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        lda_model_list.append(lda_model)\n",
    "        coherence_model = CoherenceModel(model=lda_model, texts=processed_documents, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "    optimal_model = lda_model_list[coherence_values.index(max(coherence_values))]\n",
    "    return optimal_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "섹션 번호: 100 파일: 100%|██████████| 10/10 [00:00<00:00, 1830.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/100\\2024-09-20_09.pkl\n",
      "./data/100\\2024-09-20_10.pkl\n",
      "./data/100\\2024-09-24_11.pkl\n",
      "병합한 기사 개수: 224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "섹션 번호: 101 파일: 100%|██████████| 10/10 [00:00<00:00, 1250.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/101\\2024-09-20_09.pkl\n",
      "./data/101\\2024-09-20_10.pkl\n",
      "./data/101\\2024-09-24_11.pkl\n",
      "병합한 기사 개수: 916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "섹션 번호: 102 파일: 100%|██████████| 10/10 [00:00<00:00, 1034.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/102\\2024-09-20_09.pkl\n",
      "./data/102\\2024-09-20_10.pkl\n",
      "./data/102\\2024-09-24_11.pkl\n",
      "병합한 기사 개수: 953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "섹션 번호: 103 파일: 100%|██████████| 10/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/103\\2024-09-20_09.pkl\n",
      "./data/103\\2024-09-20_10.pkl\n",
      "./data/103\\2024-09-24_11.pkl\n",
      "병합한 기사 개수: 142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "섹션 번호: 104 파일: 100%|██████████| 10/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/104\\2024-09-20_09.pkl\n",
      "./data/104\\2024-09-20_10.pkl\n",
      "./data/104\\2024-09-24_11.pkl\n",
      "병합한 기사 개수: 213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "섹션 번호: 105 파일: 100%|██████████| 10/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/105\\2024-09-20_09.pkl\n",
      "./data/105\\2024-09-20_10.pkl\n",
      "./data/105\\2024-09-24_11.pkl\n",
      "병합한 기사 개수: 114\n",
      "(0, '0.007*\"미국\" + 0.004*\"사건\" + 0.004*\"중국\" + 0.003*\"경찰\" + 0.003*\"발생\" + 0.003*\"국내\" + 0.003*\"위해\" + 0.003*\"대한\" + 0.003*\"사진\" + 0.003*\"시인\" + 0.003*\"대표\" + 0.003*\"경우\" + 0.003*\"기자\" + 0.003*\"서울\" + 0.003*\"때문\" + 0.003*\"이스라엘\" + 0.003*\"사업\" + 0.003*\"기업\" + 0.002*\"확인\" + 0.002*\"의원\" + 0.002*\"기초\" + 0.002*\"레바논\" + 0.002*\"관련\" + 0.002*\"매출\" + 0.002*\"대해\" + 0.002*\"사실\" + 0.002*\"아이폰\" + 0.002*\"이후\" + 0.002*\"여성\" + 0.002*\"사람\"')\n",
      "(1, '0.011*\"대통령\" + 0.006*\"후보\" + 0.005*\"해리스\" + 0.005*\"대해\" + 0.005*\"사람\" + 0.004*\"트럼프\" + 0.004*\"미국\" + 0.004*\"대선\" + 0.004*\"생각\" + 0.003*\"때문\" + 0.003*\"대한\" + 0.003*\"민주당\" + 0.003*\"대표\" + 0.003*\"문제\" + 0.003*\"관련\" + 0.003*\"지금\" + 0.003*\"부통령\" + 0.003*\"우크라이나\" + 0.003*\"위해\" + 0.003*\"게임\" + 0.003*\"경우\" + 0.003*\"국가\" + 0.003*\"계획\" + 0.002*\"정부\" + 0.002*\"상황\" + 0.002*\"최근\" + 0.002*\"의원\" + 0.002*\"주장\" + 0.002*\"통해\" + 0.002*\"진행\"')\n",
      "(2, '0.006*\"미국\" + 0.005*\"서울\" + 0.003*\"지역\" + 0.003*\"달러\" + 0.003*\"기자\" + 0.003*\"대한\" + 0.003*\"시장\" + 0.003*\"이스라엘\" + 0.003*\"트럼프\" + 0.003*\"결과\" + 0.003*\"전망\" + 0.003*\"경우\" + 0.003*\"상승\" + 0.003*\"최근\" + 0.003*\"오늘\" + 0.003*\"뉴스\" + 0.003*\"현재\" + 0.003*\"사진\" + 0.003*\"때문\" + 0.003*\"영향\" + 0.003*\"이후\" + 0.002*\"개발\" + 0.002*\"기온\" + 0.002*\"금리\" + 0.002*\"단백질\" + 0.002*\"섭취\" + 0.002*\"헤즈볼라\" + 0.002*\"연구\" + 0.002*\"여사\" + 0.002*\"목사\"')\n",
      "(3, '0.010*\"병원\" + 0.007*\"의료\" + 0.005*\"환자\" + 0.004*\"고려아연\" + 0.004*\"서울\" + 0.004*\"상황\" + 0.004*\"위해\" + 0.003*\"지역\" + 0.003*\"관련\" + 0.003*\"정부\" + 0.003*\"사업\" + 0.003*\"의사\" + 0.003*\"대한\" + 0.003*\"의원\" + 0.003*\"한국\" + 0.003*\"진행\" + 0.003*\"치료\" + 0.002*\"제공\" + 0.002*\"경우\" + 0.002*\"미국\" + 0.002*\"진료\" + 0.002*\"대표\" + 0.002*\"통해\" + 0.002*\"결과\" + 0.002*\"기자\" + 0.002*\"발생\" + 0.002*\"건강\" + 0.002*\"영풍\" + 0.002*\"전공\" + 0.002*\"응급\"')\n",
      "(4, '0.012*\"이스라엘\" + 0.010*\"헤즈볼라\" + 0.008*\"레바논\" + 0.006*\"대통령\" + 0.005*\"미국\" + 0.005*\"공습\" + 0.004*\"이란\" + 0.004*\"지역\" + 0.004*\"트럼프\" + 0.003*\"이후\" + 0.003*\"해리스\" + 0.003*\"이스라엘군\" + 0.003*\"남부\" + 0.003*\"대한\" + 0.003*\"위해\" + 0.003*\"전쟁\" + 0.003*\"가장\" + 0.003*\"서울\" + 0.003*\"대표\" + 0.003*\"사람\" + 0.003*\"기자\" + 0.003*\"사진\" + 0.002*\"만원\" + 0.002*\"관련\" + 0.002*\"한국\" + 0.002*\"공격\" + 0.002*\"대해\" + 0.002*\"여성\" + 0.002*\"연합뉴스\" + 0.002*\"통해\"')\n",
      "(5, '0.005*\"애플\" + 0.005*\"시장\" + 0.005*\"금리\" + 0.005*\"미국\" + 0.005*\"아이폰\" + 0.004*\"중국\" + 0.004*\"차량\" + 0.004*\"한국\" + 0.004*\"기능\" + 0.004*\"올해\" + 0.004*\"달러\" + 0.003*\"출시\" + 0.003*\"통해\" + 0.003*\"제공\" + 0.003*\"기업\" + 0.003*\"대한\" + 0.003*\"고객\" + 0.003*\"때문\" + 0.003*\"제품\" + 0.003*\"전망\" + 0.003*\"업계\" + 0.003*\"기준\" + 0.003*\"기술\" + 0.003*\"국내\" + 0.002*\"서울\" + 0.002*\"사진\" + 0.002*\"경우\" + 0.002*\"기자\" + 0.002*\"위해\" + 0.002*\"모델\"')\n"
     ]
    }
   ],
   "source": [
    "section_numbers = [100, 101, 102, 103, 104, 105]\n",
    "stopwords_file_path = './data/korean_stopwords.txt'\n",
    "\n",
    "processed_documents = process_documents(section_numbers, stopwords_file_path)\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_documents)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_documents]\n",
    "\n",
    "optimal_lda_model = optimalize_lda_model(corpus, dictionary, processed_documents)\n",
    "\n",
    "topics = optimal_lda_model.print_topics(num_words=30)\n",
    "\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 실행\n",
    "visualization = visualize_lda_model(optimal_lda_model, corpus, dictionary)\n",
    "pyLDAvis.save_html(visualization, 'optimal_lda_visualization.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic",
   "language": "python",
   "name": "topic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
