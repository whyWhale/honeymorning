{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import networkx as nx\n",
    "from itertools import zip_longest\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def load_stopwords(file_path):\n",
    "    \"\"\"\n",
    "    Load stopwords from a file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords = f.read().splitlines()\n",
    "    return stopwords\n",
    "\n",
    "def preprocess_text(text, stopwords=None):\n",
    "    \"\"\"\n",
    "    Preprocess text using Okt for LDA model input.\n",
    "    \"\"\"\n",
    "    okt = Okt()\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
    "    \n",
    "    nouns = okt.nouns(text)\n",
    "    \n",
    "    if stopwords:\n",
    "        nouns = [word for word in nouns if word not in stopwords]\n",
    "\n",
    "    return nouns\n",
    "\n",
    "def preprocess_text_with_ngrams(text, stopwords=None, n=2):\n",
    "    \"\"\"\n",
    "    Preprocess text using Okt and create n-grams for TextRank.\n",
    "    \"\"\"\n",
    "    words = preprocess_text(text, stopwords)  # Single words for LDA\n",
    "\n",
    "    # Create n-grams\n",
    "    ngrams = zip(*[words[i:] for i in range(n)])\n",
    "    ngram_list = [' '.join(ngram) for ngram in ngrams]\n",
    "    \n",
    "    return ngram_list\n",
    "\n",
    "\n",
    "def load_and_merge_section_data(section_number, hours_back=9):\n",
    "    \"\"\"\n",
    "    Load and merge content from files corresponding to the given section number.\n",
    "    \"\"\"\n",
    "    current_time = datetime.now()\n",
    "    start_time = current_time - timedelta(hours=hours_back)\n",
    "\n",
    "    valid_hours = []\n",
    "    for hour in range(hours_back + 1):\n",
    "        valid_hour = (start_time + timedelta(hours=hour)).strftime('%H')\n",
    "        valid_hours.append(valid_hour)\n",
    "\n",
    "    file_patterns = [f\"./data/{section_number}/*_{hour}.pkl\" for hour in valid_hours]\n",
    "\n",
    "    merged_content = []\n",
    "    for file_pattern in tqdm(file_patterns, desc=f\"Loading section {section_number} files\"):\n",
    "        file_list = glob.glob(file_pattern)\n",
    "\n",
    "        for file_path in file_list:\n",
    "            with open(file_path, 'rb') as file:\n",
    "                data = pickle.load(file)\n",
    "                for article in data:\n",
    "                    merged_content.append(article['content'])\n",
    "\n",
    "    merged_document = ' '.join(merged_content)\n",
    "    return merged_document\n",
    "\n",
    "\n",
    "def process_sections(section_numbers, stopwords_file_path, hours_back=9):\n",
    "    \"\"\"\n",
    "    Process each section by merging and preprocessing the data.\n",
    "    \"\"\"\n",
    "    stopwords = load_stopwords(stopwords_file_path)\n",
    "    processed_documents = {}\n",
    "\n",
    "    for section_number in tqdm(section_numbers, desc=\"Processing sections\"):\n",
    "        merged_content = load_and_merge_section_data(section_number, hours_back)\n",
    "        processed_text = preprocess_text(merged_content, stopwords)  # 단어 단위로 전처리\n",
    "        processed_documents[section_number] = processed_text\n",
    "\n",
    "    return processed_documents\n",
    "\n",
    "\n",
    "def process_sections_with_ngrams(section_numbers, stopwords_file_path, hours_back=9, n=2):\n",
    "    \"\"\"\n",
    "    Process each section by merging and preprocessing the data.\n",
    "    \"\"\"\n",
    "    stopwords = load_stopwords(stopwords_file_path)\n",
    "    processed_documents = {}\n",
    "\n",
    "    for section_number in tqdm(section_numbers, desc=\"Processing sections\"):\n",
    "        merged_content = load_and_merge_section_data(section_number, hours_back)\n",
    "        processed_text = preprocess_text_with_ngrams(merged_content, stopwords, n)\n",
    "        processed_documents[section_number] = processed_text\n",
    "\n",
    "    return processed_documents\n",
    "\n",
    "def prepare_data_for_lda(doc):\n",
    "    \"\"\"\n",
    "    Prepare data for LDA model training.\n",
    "    \"\"\"\n",
    "    dictionary = corpora.Dictionary([doc])\n",
    "    corpus = [dictionary.doc2bow(doc)]\n",
    "    return dictionary, corpus\n",
    "\n",
    "def train_lda_model(dictionary, corpus, num_topics=5, passes=15):\n",
    "    \"\"\"\n",
    "    Train the LDA model with the provided dictionary and corpus.\n",
    "    \"\"\"\n",
    "    lda_model = LdaModel(corpus=corpus,\n",
    "                         id2word=dictionary,\n",
    "                         num_topics=num_topics,\n",
    "                         random_state=100,\n",
    "                         update_every=1,\n",
    "                         passes=passes,\n",
    "                         alpha='auto',\n",
    "                         per_word_topics=True)\n",
    "    return lda_model\n",
    "\n",
    "def extract_topics_for_section(processed_docs, num_topics=1, num_words=10, passes=15):\n",
    "    \"\"\"\n",
    "    Extract topics from each section using the LDA model.\n",
    "    \"\"\"\n",
    "    section_topics = {}\n",
    "    lda_models = {}\n",
    "\n",
    "    for section, doc in tqdm(processed_docs.items(), desc=\"Extracting topics for sections\"):\n",
    "        dictionary, corpus = prepare_data_for_lda(doc)\n",
    "        lda_model = train_lda_model(dictionary, corpus, num_topics=num_topics, passes=passes)\n",
    "        topics = lda_model.show_topics(num_topics=num_topics, num_words=num_words, formatted=False)\n",
    "        section_topics[section] = topics\n",
    "        lda_models[section] = lda_model\n",
    "\n",
    "    return section_topics, lda_models\n",
    "\n",
    "def compute_relevance_with_ngrams(lda_model, topic_num, dictionary, lambda_=0.6, top_n=20):\n",
    "    \"\"\"\n",
    "    Compute relevance scores for n-grams in a specific topicAiResponseDto.\n",
    "    \"\"\"\n",
    "    topicAiResponseDto = lda_model.get_topic_terms(topic_num, topn=top_n)\n",
    "    term_frequency = dictionary.dfs\n",
    "    total_term_count = sum(term_frequency.values())\n",
    "\n",
    "    relevance_scores = []\n",
    "    for word_id, prob in topicAiResponseDto:\n",
    "        term_freq = term_frequency[word_id]\n",
    "        term_prob = term_freq / total_term_count\n",
    "        relevance = lambda_ * prob + (1 - lambda_) * (prob / term_prob)\n",
    "        relevance_scores.append((dictionary[word_id], relevance))\n",
    "\n",
    "    relevance_scores = sorted(relevance_scores, key=lambda x: x[1], reverse=True)\n",
    "    return relevance_scores\n",
    "\n",
    "def textrank_with_ngrams(relevance_scores, top_n=10):\n",
    "    \"\"\"\n",
    "    Apply TextRank on n-grams to rank keywords.\n",
    "    \"\"\"\n",
    "    word_graph = nx.Graph()\n",
    "    \n",
    "    # Check if relevance_scores is empty\n",
    "    if len(relevance_scores) == 0:\n",
    "        print(\"Relevance scores are empty. No TextRank will be applied.\")\n",
    "        return []\n",
    "\n",
    "    # Add nodes and edges only if n-grams are present\n",
    "    for i, (word1, _) in enumerate(relevance_scores):\n",
    "        word_graph.add_node(word1)\n",
    "        for word2, _ in relevance_scores[i+1:]:\n",
    "            # Ensure that we connect words that are actually in n-grams\n",
    "            if word1 != word2:\n",
    "                word_graph.add_edge(word1, word2, weight=1)\n",
    "\n",
    "    # Ensure the graph is not empty\n",
    "    if len(word_graph.nodes) == 0:\n",
    "        print(\"No edges were added to the graph. No TextRank will be applied.\")\n",
    "        return []\n",
    "\n",
    "    scores = nx.pagerank(word_graph)\n",
    "    ranked_keywords = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    \n",
    "    # Check the output\n",
    "    if len(ranked_keywords) == 0:\n",
    "        print(\"TextRank resulted in no ranked keywords.\")\n",
    "    \n",
    "    return ranked_keywords\n",
    "\n",
    "def generate_phrases_from_ngrams(ranked_keywords, relevance_scores, n=2):\n",
    "    \"\"\"\n",
    "    Generate meaningful phrases from ranked keywords using n-grams.\n",
    "    \"\"\"\n",
    "    phrases = []\n",
    "    used_keywords = set()\n",
    "\n",
    "    if len(ranked_keywords) == 0:\n",
    "        print(\"Ranked keywords are empty. No phrases will be generated.\")\n",
    "        return []\n",
    "\n",
    "    for i, (word1, _) in enumerate(relevance_scores):\n",
    "        if word1 in used_keywords:\n",
    "            continue\n",
    "        phrase = [word1]\n",
    "        for word2, _ in relevance_scores[i+1:]:\n",
    "            if word2 not in used_keywords and word2 in [w for w, _ in ranked_keywords]:\n",
    "                if len(phrase) < n:\n",
    "                    phrase.append(word2)\n",
    "                    used_keywords.add(word2)\n",
    "                else:\n",
    "                    break\n",
    "        used_keywords.add(word1)\n",
    "        if len(phrase) == n:\n",
    "            phrases.append(' '.join(phrase))\n",
    "\n",
    "    if len(phrases) == 0:\n",
    "        print(\"No phrases were generated from n-grams.\")\n",
    "    \n",
    "    return phrases[:top_n]\n",
    "\n",
    "\n",
    "def finalize_labels_with_details(lda_models, section_topics, processed_docs, top_n=5, top_topic_words=5):\n",
    "    \"\"\"\n",
    "    Finalize labels for each section based on LDA and TextRank results.\n",
    "    Outputs a structured summary of topics and their corresponding labels and words.\n",
    "    \"\"\"\n",
    "    final_details = []\n",
    "\n",
    "    for section, topics in section_topics.items():\n",
    "        lda_model = lda_models[section]\n",
    "        dictionary = lda_model.id2word\n",
    "\n",
    "        for topic_num, _ in enumerate(topics):\n",
    "            # LDA 모델의 상위 단어들 추출\n",
    "            topic_words_lda = [dictionary[word_id] for word_id, _ in lda_model.get_topic_terms(topic_num, topn=top_topic_words)]\n",
    "\n",
    "            # Relevance 기반으로 상위 단어들 추출\n",
    "            relevance_scores = compute_relevance_with_ngrams(lda_model, topic_num, dictionary)\n",
    "            topic_words_relevance = [word for word, _ in relevance_scores[:top_topic_words]]\n",
    "\n",
    "            # TextRank 기반으로 상위 레이블 생성\n",
    "            ngrams = preprocess_text_with_ngrams(' '.join(processed_docs[section]), stopwords=None, n=2)\n",
    "            ranked_keywords = textrank_with_ngrams([(ngram, 1.0) for ngram in ngrams], top_n)\n",
    "            labels_textrank = generate_phrases_from_ngrams(ranked_keywords, relevance_scores, n=2)\n",
    "\n",
    "            # 각 토픽에 대해 LDA, Relevance, TextRank 기반의 단어와 레이블들을 저장\n",
    "            final_details.append({\n",
    "                'Section': section,\n",
    "                'Topic Number': topic_num,\n",
    "                'LDA Top Words': ', '.join(topic_words_lda),\n",
    "                'Relevance Top Words': ', '.join(topic_words_relevance),\n",
    "                'TextRank Labels': ', '.join(labels_textrank)\n",
    "            })\n",
    "\n",
    "    return final_details\n",
    "\n",
    "\n",
    "def display_final_table(final_details):\n",
    "    \"\"\"\n",
    "    Display the final table of LDA, Relevance, and TextRank results.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(final_details)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "\n",
    "# 1. 불용어 파일 경로 설정\n",
    "stopwords_file_path = './data/korean_stopwords.txt'\n",
    "\n",
    "# 2. 섹션 번호 설정\n",
    "section_numbers = [100, 101, 102]  # 처리할 섹션 번호 리스트\n",
    "hours_back = 9  # 최근 9시간의 데이터만 사용\n",
    "\n",
    "# 3. 섹션 데이터 병합 및 전처리 (단어 단위)\n",
    "processed_docs = process_sections(section_numbers, stopwords_file_path, hours_back)\n",
    "\n",
    "# 4. LDA 모델을 사용하여 각 섹션별로 토픽 추출\n",
    "num_topics = 3  # 각 섹션에서 추출할 토픽의 수\n",
    "num_words = 10  # 각 토픽에서 추출할 상위 단어 수\n",
    "section_topics, lda_models = extract_topics_for_section(processed_docs, num_topics, num_words)\n",
    "\n",
    "# 5. 최종 토픽 레이블 및 세부 사항 생성\n",
    "top_n = 5  # 각 토픽에 대해 생성할 레이블의 수\n",
    "top_topic_words = 5  # 각 토픽에 대해 추출할 상위 단어의 수\n",
    "final_details = finalize_labels_with_details(lda_models, section_topics, processed_docs, top_n, top_topic_words)\n",
    "\n",
    "# 6. 결과 표로 출력\n",
    "df = display_final_table(final_details)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
