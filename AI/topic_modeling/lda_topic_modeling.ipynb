{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c6cf5e9-9489-4f51-bb30-4377c443e203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stopwords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sections:   0%|          | 0/6 [00:00<?, ?it/s]\n",
      "섹션 번호: 100 파일: 100%|██████████| 10/10 [00:00<00:00, 3641.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100번 텍스트 전처리 중입니다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing sections:  17%|█▋        | 1/6 [00:08<00:43,  8.78s/it]\n",
      "섹션 번호: 101 파일: 100%|██████████| 10/10 [00:00<00:00, 2162.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101번 텍스트 전처리 중입니다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sections:  33%|███▎      | 2/6 [00:24<00:51, 12.92s/it]\n",
      "섹션 번호: 102 파일: 100%|██████████| 10/10 [00:00<00:00, 1322.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102번 텍스트 전처리 중입니다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sections:  50%|█████     | 3/6 [00:51<00:58, 19.43s/it]\n",
      "섹션 번호: 103 파일: 100%|██████████| 10/10 [00:00<00:00, 5141.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103번 텍스트 전처리 중입니다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sections:  67%|██████▋   | 4/6 [00:56<00:27, 13.71s/it]\n",
      "섹션 번호: 104 파일: 100%|██████████| 10/10 [00:00<00:00, 5057.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104번 텍스트 전처리 중입니다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sections:  83%|████████▎ | 5/6 [01:00<00:10, 10.10s/it]\n",
      "섹션 번호: 105 파일: 100%|██████████| 10/10 [00:00<00:00, 5500.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105번 텍스트 전처리 중입니다\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sections: 100%|██████████| 6/6 [01:04<00:00, 10.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 100: 불사 현장 비례 요청 거부 이준석 개혁 신당 의원 김영선 국민 의원 뉴스 서울 뉴스 박태훈 선임 기자 이준석 개혁 신당 의원 김건희 여사 공천 개입 의혹 대해 입장 정리 발표 의원 취재 요청 통해 주요 쟁점 대한 생각 경남 하동 불사 김영선 국민 의원 동한 대해 의원 오전 김영선 의원 관계자 의원 직접 보라 종용 김영선 의원 서울 오기 새벽 불사 도착 대화 새벽 불사 서울 불사 당시 상황 대해 텔레 그램 문자 내용 완결 의원 비례대표 달라 의원 측요구 현장 거부 오전 해당 관계자 생각 재차 오후 금태섭 의원 종로 선거 사무소 개소식 개혁 신당 관계자 해당 내용 공유 결과 부정 김영선 의원 김종인 관리 위원장 가족 비례대표 공천 이야기 개혁 신당 비례대표 공천 약속 방증 주장 뉴스 토마토 텔레 그램 캡처 가지 내용 불확실 성도 계속 이야기 보도 강행 의원 대표 시절 창원 의창 보궐선거 김영선 의원 공천 대해 당시 윤상현 관리 위원장 보궐선거 공천 일임 관여 의원 공천 이유 관위 경상남\n",
      "Section 101: 지난 오후 서울 여의도 한국 거래소 전광판 하이닉스 종가 뉴스 김영 미래에셋 증권 연구원 하이닉스 인공 지능 핵심 반도체 대역폭 메모리 수요 성장 지속 판단 연구원 하이닉스 대한 투자 의견 매수 목표 주가 만원 유지 겨울 가장 겨울 표현 미국 투자 은행 모건스탠리 보고서 염두 모건스탠리 추석 연휴 지난 겨울 제목 보고서 하이닉스 대한 투자 의견 비중 축소 단계 목표 가도 만원 만원 하향 조정 메모리 반도체 수요 정점 역시 공급 과잉 가능성 이유 모건스탠리 보고서 여파 전날 하이닉스 주가 만원 낙폭 기록 연구원 하이닉스 올해 분기 제품 하량 시장 대치 밑돌 예상 연구원 하이닉스 최근 주가 하락 업황 의미 변화 관성 사고 급진 수익 실현 결과 연구원 하이닉스 수익 예상 하이닉스 노드 성능 검증 노드 개발 연내 마무리 전망 경쟁 안정 공정 미세 화로 성능 제조 원가 측면 우위 지속 가능성 연구원 세대 경우 개발 막바지 단계 분기 출하 엔비디아 차세대 반도체 블랙웰 본격 탑재 라며 역시 년\n",
      "Section 102: 대구대 생물 교육 차수 현씨 아버지 명예 졸업장 전달 사진 대구대 파이낸셜뉴스 지병 세상 아르바이트 후배 장학금 대구대 생물 교육 수현 명예 졸업장 대구대 오후 경산 캠퍼스 성산 명예 졸업장 전달 수현 아버지 차민수 명예 졸업장 학교 측은 수현 대학 기탁 장학금 전달 식도 후배 만원 장학금 예정 수현 지난 나이 대장암 세상 평소 아르바이트 만원 교사 후배 장학금 사실 감동 당시 대구대 수현 평소 사범 대학 건물 벤치 추모 문구 명예 졸업장 전달 리기 아버지 차씨 학년 교생 실습 몹시 끝내 교사 명예 졸업장 모습 하늘 무척 생각 박순진 대구대 총장 투병 학업 포기 수현 학생 열정 헌신 학생 귀감 수현 학생 열정 기억 장학금 대구대 명예 졸업장 수현 기사 이해 돕기 자료 이미지 사진 이미지 뱅크 산책 반려견 길고양이 물어 조치 남성 입건 경찰 지난 오전 성남시 수정구 금토 동의 사업 관계자 여간 길고양이 견주 산책 강아지 내용 신고 접수 경기 수정 경찰서 조사 진행 중이 현장 진돗개 강\n",
      "Section 103: 당뇨 식단 헬스 조선 식품 영양성분 데이터베이스 소고기 등심 스테이크 안심 스테이크 열량 섭취 열량 조절 필수 당뇨병 환자 위해 맞춤 스테이크 레시피 준비 양배추 스테이크 분위기 식사 강남 세브란스병원 당뇨 식단 오늘 추천 레시피 배달 양배추 스테이크 양배추 스테이크 열량 아침 점심 저녁 걱정 고요 양배추 기름 특유 단맛 추가 조리법 버터 구우 양배추 전체 미가 기술 식감 유지 재미 달라 일당백 재료 양배추 핵심 재료 양배추 영양 효능 채소 비타민 비타민 함유 위장 보호 설포 성분 면역 칼로리 섬유질 포만 유지 혈당 항산화 기능 카로티노이드 성분 피부 노화 상피세포 재생 피부 개선 효과 아스파라거스 아스파라거스 칼륨 함량 체내 노폐물 배출 비타민 당뇨병 안과 질환 예방 효과 아스파라긴산 성분 신진대사 촉진 피로 해소 단백질 합성 체력 향상 단백질 새송이 충전 송이버섯 우리나라 고유 품종 단백질 함량 송이버섯 단백질 함량 아스파라거스 양배추 송이버섯 비타민 비타민 신진대사 촉진 면역 \n",
      "Section 104: 증권 규모 환수 벌금 가상 화폐 테라 루나 사태 핵심 인물 권도형 테라 폼랩스 대표 몬테네그로 포드고리차 경찰청 조사 경찰관 포드고리차 뉴시스 미국 법원 가상 화폐 테라 루나 발행 사인 테라폼랩스 파산 승인 로이터 통신 미국 델라웨어주 파산 법원 추가 소송 환영 안이 라며 테라폼랩스 파산 계획 승인 보도 테라 폼랩스 측은 파산 청산 통해 일부 암호 화폐 구매 억만억만 달러 억억 사이 금액 지급 추산 테라폼랩스 미국 증권 거래 위원회 제기 민사 소송 억만 달러 조억 규모 환수 벌금 납부 합의 파산 계획 승인 환수 벌금 가능성 테라폼스랩 파산 청산 과정 투자자 손실 먼저 배상 환수 내기 동의 손실 배상 체적 관측 테라 폼랩스 측은 현재 보상 자격 가상 화폐 손실 전체 금액 추정 앞서 권도형 테라 폼랩스 대표 테라폼랩스 테라 안정 성과 관련 투자자 거액 투자 손실 민사 소송 제기 당시 사기피해 금액 최소 달러 조억 원로 추산 연관 기사 테라 권도형 촉발 몬테네그로 대통령 총리 갈등 노선 \n",
      "Section 105: 애플 전문 분석 궈밍치 전망 애플 내년 이후 출시 아이폰 아이폰 시리즈 탑재 프로세서 대한 정보 아레나 주요 외신 애플 전문 분석 궈밍치 전망 인용 애플 내년 출시 아이폰 시리즈 나노 공정 기반 프로세서 탑재 보도 아이폰 프로 사진 애플 궈밍치 엑스 통해 내년 출시 아이폰 나노 탑재 출시 아이폰 모델 프로세서 나노 공정 기술 사용 예정 비용 문제 아이폰 일부 모델 나노 장착 지난 애플 회동 나노 공급 예약 보도 때문 일부 아이폰 시리즈 나노 탑재 팩터 처리 속도 전력 효율 궈밍치 전망 라면 나노 공정 내후년 일부 아이폰 모델 도입 예정 작년 애플 아이폰 제품 나노 공정 탑재 아이폰 프로 모델 프로 시리즈 나노 공정 기반 제작 올해 출시 아이폰 시리즈 세대 나노 공정 사용 제작 사용 작년 아이폰 사용 바이오 효율 매체 맥루머스 후반 나노 생산 시작 계획 애플 공정 제작 공급 회사 예상 나노 생산 위해 대만 남무 가오슝 신규 공장 건설 중이 시설 대한 승인 위해 노력 중이 나혼렙 국내 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "from konlpy.tag import Okt\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_stopwords(file_path):\n",
    "    \"\"\"\n",
    "    주어진 경로에서 불용어를 불러오는 함수\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords = f.read().splitlines()\n",
    "    return stopwords\n",
    "\n",
    "def preprocess_text(text, stopwords=None):\n",
    "    \"\"\"\n",
    "    Okt를 사용한 텍스트 전처리 함수\n",
    "    - 불용어 제거\n",
    "    - 명사 추출\n",
    "    \"\"\"\n",
    "    okt = Okt()\n",
    "    text = re.sub(r'\\d+', '', text)  # 숫자 제거\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # 특수문자 제거\n",
    "    \n",
    "    nouns = okt.nouns(text)\n",
    "    \n",
    "    if stopwords:\n",
    "        nouns = [word for word in nouns if word not in stopwords]\n",
    "    \n",
    "    nouns = [word for word in nouns if len(word) > 1]\n",
    "    \n",
    "    processed_text = ' '.join(nouns)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "def load_and_merge_section_data(section_number, hours_back=9):\n",
    "    \"\"\"\n",
    "    섹션 번호에 해당하는 모든 pkl 파일을 불러와서 기사 본문을 하나의 변수로 병합하는 함수\n",
    "    현재 시간에서 -hours_back 시간 전까지의 파일만 병합\n",
    "    \"\"\"\n",
    "    current_time = datetime.now()\n",
    "    start_time = current_time - timedelta(hours=hours_back)\n",
    "    \n",
    "    valid_hours = []\n",
    "    for hour in range(hours_back + 1):  # 0부터 hours_back까지의 시간을 계산\n",
    "        valid_hour = (start_time + timedelta(hours=hour)).strftime('%H')\n",
    "        valid_hours.append(valid_hour)\n",
    "    \n",
    "    file_patterns = [f\"../data/{section_number}/*_{hour}.pkl\" for hour in valid_hours]\n",
    "    \n",
    "    merged_content = []\n",
    "    for file_pattern in tqdm(file_patterns, desc=f\"섹션 번호: {section_number} 파일\"):\n",
    "        file_list = glob.glob(file_pattern)\n",
    "        \n",
    "        for file_path in file_list:\n",
    "            with open(file_path, 'rb') as file:\n",
    "                data = pickle.load(file)\n",
    "                for article in data:\n",
    "                    merged_content.append(article['content'])\n",
    "    \n",
    "    merged_document = ' '.join(merged_content)\n",
    "    \n",
    "    return merged_document\n",
    "\n",
    "def process_sections(section_numbers, stopwords_file_path, hours_back=9):\n",
    "    \"\"\"\n",
    "    주어진 섹션 번호 리스트에 대해 각 섹션별로 데이터를 병합하고 전처리하는 함수\n",
    "    \"\"\"\n",
    "    # 불용어 로드\n",
    "    print(\"Loading stopwords...\")\n",
    "    stopwords = load_stopwords(stopwords_file_path)\n",
    "    \n",
    "    processed_documents = {}\n",
    "    \n",
    "    for section_number in tqdm(section_numbers, desc=\"Processing sections\"):\n",
    "        # 데이터 병합\n",
    "        merged_content = load_and_merge_section_data(section_number, hours_back)\n",
    "        \n",
    "        # 텍스트 전처리\n",
    "        print(f\"{section_number}번 텍스트 전처리 중...\")\n",
    "        processed_text = preprocess_text(merged_content, stopwords)\n",
    "        \n",
    "        # 섹션 번호를 키로 하여 전처리된 문서를 저장\n",
    "        processed_documents[section_number] = processed_text\n",
    "    \n",
    "    return processed_documents\n",
    "\n",
    "# 사용 예시\n",
    "section_numbers = [100, 101, 102, 103, 104, 105]\n",
    "stopwords_file_path = '../data/korean_stopwords.txt'\n",
    "processed_docs = process_sections(section_numbers, stopwords_file_path)\n",
    "\n",
    "# 전처리된 각 섹션의 첫 500자를 출력 (확인용)\n",
    "for section, doc in processed_docs.items():\n",
    "    print(f\"Section {section}: {doc[:500]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a058687-2fca-4a66-9411-211d1040dee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25dc2300-b4ef-4f4c-b0aa-0e5ae914ec65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Documents: 100%|██████████| 6/6 [00:00<00:00, 252.25it/s]\n",
      "Creating Corpus: 100%|██████████| 6/6 [00:00<00:00, 81.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LDA Model...\n",
      "토픽 추출 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Displaying Topics: 100%|██████████| 5/5 [00:00<00:00, 42974.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: ['고려아연', '서울', '상승', '시장', '투자', '파트너', '금리', '기업', '미국', '가격']\n",
      "Topic 1: ['미국', '대통령', '이스라엘', '해리스', '트럼프', '후보', '부통령', '중국', '지난', '대선']\n",
      "Topic 2: ['의원', '대통령', '국민', '여사', '민주당', '대표', '북한', '김건희', '공천', '지금']\n",
      "Topic 3: ['의원', '대통령', '국민', '민주당', '여사', '대표', '서울', '대해', '대한', '관련']\n",
      "Topic 4: ['경찰', '지난', '병원', '서울', '지역', '혐의', '위해', '기자', '사진', '대한']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "def prepare_data_for_lda(processed_docs):\n",
    "    \"\"\"\n",
    "    LDA를 위한 데이터를 준비하는 함수\n",
    "    - processed_docs: 섹션별 전처리된 텍스트 데이터 (딕셔너리 형태)\n",
    "    \"\"\"\n",
    "    # 각 섹션의 전처리된 문서를 리스트로 변환\n",
    "    texts = []\n",
    "    for doc in tqdm(processed_docs.values(), desc=\"Processing Documents\"):\n",
    "        texts.append(doc.split())\n",
    "    \n",
    "    # 딕셔너리 생성\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "    # 코퍼스 생성 (단어의 빈도수로 변환된 텍스트 데이터)\n",
    "    corpus = [dictionary.doc2bow(text) for text in tqdm(texts, desc=\"Creating Corpus\")]\n",
    "    \n",
    "    return dictionary, corpus\n",
    "\n",
    "def train_lda_model(dictionary, corpus, num_topics=5, passes=15):\n",
    "    \"\"\"\n",
    "    LDA 모델을 학습하는 함수\n",
    "    - dictionary: Gensim의 Dictionary 객체\n",
    "    - corpus: Gensim의 코퍼스 객체\n",
    "    - num_topics: 생성할 토픽의 수\n",
    "    - passes: 학습 반복 횟수\n",
    "    \"\"\"\n",
    "    # LDA 모델 학습\n",
    "    print(\"Training LDA Model...\")\n",
    "    lda_model = LdaModel(corpus=corpus,\n",
    "                         id2word=dictionary,\n",
    "                         num_topics=num_topics,\n",
    "                         random_state=100,\n",
    "                         update_every=1,\n",
    "                         passes=passes,\n",
    "                         alpha='auto',\n",
    "                         per_word_topics=True)\n",
    "    \n",
    "    return lda_model\n",
    "\n",
    "def print_lda_topics(lda_model, num_words=10):\n",
    "    \"\"\"\n",
    "    LDA 모델로부터 생성된 토픽들을 출력하는 함수\n",
    "    - lda_model: 학습된 LDA 모델\n",
    "    - num_words: 각 토픽에 대해 출력할 단어 수\n",
    "    \"\"\"\n",
    "    print(\"토픽 추출 중...\")\n",
    "    topics = lda_model.show_topics(num_topics=-1, num_words=num_words, formatted=False)\n",
    "    for i, topic in enumerate(tqdm(topics, desc=\"Displaying Topics\")):\n",
    "        topic_words = [word for word, _ in topic[1]]  # topic[1]이 단어-가중치 리스트임\n",
    "        print(f\"Topic {i}: {topic_words}\")\n",
    "\n",
    "# 사용 예시\n",
    "dictionary, corpus = prepare_data_for_lda(processed_docs)\n",
    "lda_model = train_lda_model(dictionary, corpus, num_topics=5, passes=15)\n",
    "print_lda_topics(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543e6088-d57c-4b0c-965b-323263e98f21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
